{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BITEXT FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2bitext(file_input_name,word_before,file_output_name):\n",
    "#     f = open('train.words.txt', 'rt')\n",
    "   \n",
    "    #***************** READ FILE AND BUILD LINES*****************\n",
    "    #count number of lines in the file\n",
    "    f = open(file_input_name, 'rt')\n",
    "    cpt_file=0\n",
    "    for line in f:\n",
    "        cpt_file+=1\n",
    "    f.close()\n",
    "    print(\"number of lines in the file\", cpt_file)\n",
    "\n",
    "    #build the sentences\n",
    "    word_before=''\n",
    "    sentence=[]\n",
    "    cpt_word_line=[]\n",
    "    all_sentences=[]\n",
    "    cpt=0\n",
    "    cpt_all=0\n",
    "\n",
    "    f = open(file_input_name, 'rt')\n",
    "    for line in f:\n",
    "        if len(line.strip()) == 0 and word_before==word_before:\n",
    "            all_sentences.append(' '.join(sentence))\n",
    "            cpt_word_line.append(cpt)\n",
    "            sentence=[]\n",
    "            cpt=0\n",
    "            cpt_all+=1\n",
    "        if len(line.strip()) >0:\n",
    "            sentence.append(line.strip())\n",
    "            word_before=line.strip()\n",
    "            cpt+=1\n",
    "            cpt_all+=1\n",
    "        if cpt_all==cpt_file:\n",
    "            all_sentences.append(' '.join(sentence)) \n",
    "            cpt_word_line.append(cpt)\n",
    "    f.close()\n",
    "    \n",
    "    #***************** SAVE LINES TO OUTPUT_FILE *****************\n",
    "    save_to_txt(all_sentences,file_output_name)\n",
    "#     with open(file_output_name, \"a\") as a_file:\n",
    "#         for elem in all_sentences:\n",
    "#             a_file.write(elem)\n",
    "#             a_file.write(\"\\n\")\n",
    "    \n",
    "#     a_file.close()\n",
    "    \n",
    "def save_to_txt(sentences_array,file_output_name):\n",
    "    if os.path.isfile(file_output_name):\n",
    "        os.remove(file_output_name)\n",
    "    with open(file_output_name, \"a\") as a_file:\n",
    "        for elem in sentences_array:\n",
    "            a_file.write(elem)\n",
    "            a_file.write(\"\\n\")\n",
    "    a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines in the file 220663\n",
      "number of lines in the file 220663\n"
     ]
    }
   ],
   "source": [
    "file2bitext(\"train.words.txt\",\".\",\"train_org_words_bitext.txt\")\n",
    "file2bitext(\"train.tags.txt\",\".\",\"train_org_tags_bitext.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines in the file 49389\n",
      "number of lines in the file 49389\n"
     ]
    }
   ],
   "source": [
    "file2bitext(\"test.words.txt\",\".\",\"test_words_bitext.txt\")\n",
    "file2bitext(\"test.tags.txt\",\".\",\"test_tags_bitext.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALID SET"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "the valid set should be an extract of 1000 lines from the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8937\n"
     ]
    }
   ],
   "source": [
    "def get_all_sentences_from_txt(file_name):\n",
    "    sentences=[]\n",
    "    f = open(file_name, 'rt')\n",
    "    sentences_words=[]\n",
    "    for line in f:\n",
    "        sentences.append(line)\n",
    "    f.close()\n",
    "    return sentences\n",
    "\n",
    "train_words=get_all_sentences_from_txt(\"train_org_words_bitext.txt\")\n",
    "train_tags=get_all_sentences_from_txt(\"train_org_tags_bitext.txt\")\n",
    "\n",
    "train_words=[elem[:-1] for elem in train_words]\n",
    "train_tags=[elem[:-1] for elem in train_tags]\n",
    "print(len(train_words))\n",
    "\n",
    "#index list 1000 values\n",
    "# list_valid=np.random.random_integers(0,len(train_words)-1,1000)\n",
    "list_valid=np.random.randint(0,len(train_words)-1,1000)\n",
    "\n",
    "valid_words=[]\n",
    "valid_tags=[]\n",
    "for idx in list_valid:\n",
    "    valid_words.append(train_words[idx])\n",
    "    valid_tags.append(train_tags[idx])\n",
    "\n",
    "save_to_txt(valid_words,\"valid_words_bitext.txt\")\n",
    "save_to_txt(valid_tags,\"valid_tags_bitext.txt\")\n",
    "\n",
    "#the new train without 1000 lines\n",
    "for idx in sorted(list_valid, reverse=True):\n",
    "    train_words.pop(idx)\n",
    "    train_tags.pop(idx)\n",
    "\n",
    "save_to_txt(train_words,\"train_words_bitext.txt\")\n",
    "save_to_txt(train_tags,\"train_tags_bitext.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# len(np.random.random_integers(0,7937,1000))\n",
    "# sorted(np.random.randint(0, 8936,1000),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onmt-build-vocab --size 50000 --save_vocab src-train-vocab.txt train_words_bitext.txt\n",
    "onmt-build-vocab --tokenizer_config char_tokenization.yml --size 100 --save_vocab src-train-tkt-vocab.txt train_words_bitext.txt\n",
    "onmt-build-vocab --size 100 --save_vocab tgt-train-vocab.txt train_tags_bitext.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ wc -l train_org_words_bitext.txt train_org_tags_bitext.txt train_words_bitext.txt train_tags_bitext.txt valid_words_bitext.txt valid_tags_bitext.txt src-train-vocab.txt tgt-train-vocab.txt src-train-tkt-vocab.txt test_words_bitext.txt test_tags_bitext.txt\n",
    "   8937 train_org_words_bitext.txt\n",
    "   8937 train_org_tags_bitext.txt\n",
    "   7937 train_words_bitext.txt\n",
    "   7937 train_tags_bitext.txt\n",
    "   1000 valid_words_bitext.txt\n",
    "   1000 valid_tags_bitext.txt\n",
    "  17977 src-train-vocab.txt\n",
    "     43 tgt-train-vocab.txt\n",
    "     84 src-train-tkt-vocab.txt\n",
    "   2013 test_words_bitext.txt\n",
    "   2013 test_tags_bitext.txt\n",
    "  57878 total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hanane@LAPTOP-79FP3JFQ MINGW64 /C/Users/Hanane/Documents/Python_Scripts/TelecomParis/Qwant/TP/transformation_files\n",
    "$ onmt-main --model_type SequenceTagger --config data.yml --auto_config train --with_eval\n",
    "2020-06-16 16:16:26.069688: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library                                                   'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
    "2020-06-16 16:16:26.070365: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not                                                   have a GPU set up on your machine.\n",
    "WARNING:root:Could not import signal.SIGPIPE (this is expected on Windows machines)\n",
    "usage: onmt-main [-h] [-v] --config CONFIG [CONFIG ...] [--auto_config]\n",
    "                 [--model_type {GPT2Small,ListenAttendSpell,LstmCnnCrfTagger,LuongAttention,NMTBigV1,NMTMediumV1,NMTSmallV                                                  1,Transformer,TransformerBase,TransformerBaseRelative,TransformerBig,TransformerBigRelative,TransformerRelative}]\n",
    "                 [--model MODEL] [--run_dir RUN_DIR] [--data_dir DATA_DIR]\n",
    "                 [--checkpoint_path CHECKPOINT_PATH]\n",
    "                 [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}]\n",
    "                 [--seed SEED] [--gpu_allow_growth]\n",
    "                 [--intra_op_parallelism_threads INTRA_OP_PARALLELISM_THREADS]\n",
    "                 [--inter_op_parallelism_threads INTER_OP_PARALLELISM_THREADS]\n",
    "                 [--mixed_precision]\n",
    "                 {train,eval,infer,export,score,average_checkpoints,update_vocab}\n",
    "                 ...\n",
    "onmt-main: error: argument --model_type: invalid choice: 'SequenceTagger' (choose from 'GPT2Small', 'ListenAttendSpell', '                                                  LstmCnnCrfTagger', 'LuongAttention', 'NMTBigV1', 'NMTMediumV1', 'NMTSmallV1', 'Transformer', 'TransformerBase', 'Transform                                                  erBaseRelative', 'TransformerBig', 'TransformerBigRelative', 'TransformerRelative')\n",
    "\n",
    "Hanane@LAPTOP-79FP3JFQ MINGW64 /C/Users/Hanane/Documents/Python_Scripts/TelecomParis/Qwant/TP/transformation_files\n",
    "$ onmt-main --model_type LstmCnnCrfTagger --config data.yml --auto_config train --with_eval\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
