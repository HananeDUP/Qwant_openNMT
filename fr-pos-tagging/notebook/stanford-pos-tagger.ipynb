{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"C:\\\\Users\\\\Hanane\\\\Documents\\\\Python_Scripts\\\\TelecomParis\\\\Qwant\\\\dataset\\\\big_file_split\"\n",
    "file_name=\"month_2019_11_1.json\"\n",
    "df=pd.read_json(Path + \"\\\\\"+file_name, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 659 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metaData</th>\n",
       "      <th>hashSignature</th>\n",
       "      <th>title</th>\n",
       "      <th>region</th>\n",
       "      <th>domain</th>\n",
       "      <th>media</th>\n",
       "      <th>mobile</th>\n",
       "      <th>index_date</th>\n",
       "      <th>category</th>\n",
       "      <th>boost</th>\n",
       "      <th>...</th>\n",
       "      <th>published_date</th>\n",
       "      <th>description</th>\n",
       "      <th>body</th>\n",
       "      <th>host</th>\n",
       "      <th>qrank</th>\n",
       "      <th>country</th>\n",
       "      <th>url</th>\n",
       "      <th>lastmod</th>\n",
       "      <th>slug</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>Comment berner un système de reconnaissance d’...</td>\n",
       "      <td></td>\n",
       "      <td>sciencesetavenir.fr</td>\n",
       "      <td>https://www.sciencesetavenir.fr/assets/img/201...</td>\n",
       "      <td>https://www.sciencesetavenir.fr/high-tech/inte...</td>\n",
       "      <td>1572792800</td>\n",
       "      <td>tech</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1572789600</td>\n",
       "      <td>Faire passer un panda pour un gibbon, transfor...</td>\n",
       "      <td>Comment berner un système de reconnaissance d’...</td>\n",
       "      <td>sciencesetavenir.fr</td>\n",
       "      <td>14231</td>\n",
       "      <td>fr</td>\n",
       "      <td>https://www.sciencesetavenir.fr/high-tech/inte...</td>\n",
       "      <td>2019-11-03 14:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Comment berner un système de reconnaissance d’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>Le grand huit de Christopher Bell au Texas</td>\n",
       "      <td></td>\n",
       "      <td>us-racing.com</td>\n",
       "      <td>http://www.us-racing.com/wp-content/uploads/20...</td>\n",
       "      <td>http://www.us-racing.com/2019/11/03/le-grand-h...</td>\n",
       "      <td>1572792801</td>\n",
       "      <td>sports</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1572788997</td>\n",
       "      <td>FORT WORTH, TEXAS – NOVEMBER 02: Christopher B...</td>\n",
       "      <td>NHRA En remportant sa huitième victoire de la ...</td>\n",
       "      <td>us-racing.com</td>\n",
       "      <td>408501</td>\n",
       "      <td>fr</td>\n",
       "      <td>http://www.us-racing.com/2019/11/03/le-grand-h...</td>\n",
       "      <td>2019-11-03 14:02:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Le grand huit de Christopher Bell au Texas. NH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  metaData hashSignature                                              title  \\\n",
       "0       {}            []  Comment berner un système de reconnaissance d’...   \n",
       "1       {}            []         Le grand huit de Christopher Bell au Texas   \n",
       "\n",
       "  region               domain  \\\n",
       "0         sciencesetavenir.fr   \n",
       "1               us-racing.com   \n",
       "\n",
       "                                               media  \\\n",
       "0  https://www.sciencesetavenir.fr/assets/img/201...   \n",
       "1  http://www.us-racing.com/wp-content/uploads/20...   \n",
       "\n",
       "                                              mobile  index_date category  \\\n",
       "0  https://www.sciencesetavenir.fr/high-tech/inte...  1572792800     tech   \n",
       "1  http://www.us-racing.com/2019/11/03/le-grand-h...  1572792801   sports   \n",
       "\n",
       "   boost  ...  published_date  \\\n",
       "0    1.0  ...      1572789600   \n",
       "1    1.0  ...      1572788997   \n",
       "\n",
       "                                         description  \\\n",
       "0  Faire passer un panda pour un gibbon, transfor...   \n",
       "1  FORT WORTH, TEXAS – NOVEMBER 02: Christopher B...   \n",
       "\n",
       "                                                body                 host  \\\n",
       "0  Comment berner un système de reconnaissance d’...  sciencesetavenir.fr   \n",
       "1  NHRA En remportant sa huitième victoire de la ...        us-racing.com   \n",
       "\n",
       "    qrank  country                                                url  \\\n",
       "0   14231       fr  https://www.sciencesetavenir.fr/high-tech/inte...   \n",
       "1  408501       fr  http://www.us-racing.com/2019/11/03/le-grand-h...   \n",
       "\n",
       "               lastmod slug                                               text  \n",
       "0  2019-11-03 14:00:00  NaN  Comment berner un système de reconnaissance d’...  \n",
       "1  2019-11-03 14:02:38  NaN  Le grand huit de Christopher Bell au Texas. NH...  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "source=pd.DataFrame.from_records(df['_source'])\n",
    "source['text']=source['title']+\". \"+source['body']\n",
    "source.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nltk and Stanford pos-tagging example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Comment', 'berner', 'un', 'système', 'de', 'reconnaissance', 'd', '’', 'image', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# file_content = open(\"myfile.txt\").read()\n",
    "text_expample = source['title'][0]\n",
    "tokens = nltk.word_tokenize(text_expample)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('je', 'PRON'), ('suis', 'AUX'), ('libre', 'ADJ')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordPOSTagger\n",
    "jar = 'C:/Users/Hanane/Documents/nltk_stanford_pos_tagging/stanford-tagger-4.0.0/stanford-postagger-4.0.0.jar'\n",
    "model = 'C:/Users/Hanane/Documents/nltk_stanford_pos_tagging/stanford-tagger-4.0.0/models/french-ud.tagger'\n",
    "import os\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_121/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding='utf8' )\n",
    "res = pos_tagger.tag('je suis libre'.split())\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagset = None\n",
    "tokens = nltk.word_tokenize('the mat sat on the cat')\n",
    "tags = nltk.tag._pos_tag(tokens, tagset, tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['//coupdecoeur.ca/programmation/montreal/philemon-cimon-jerome-miniere-solo-141119/']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('//coupdecoeur.ca/programmation/montreal/philemon-cimon-jerome-miniere-solo-141119/',\n",
       "  'NUM')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_expample = \"l\\'appareil\"\n",
    "text_expample=\"//coupdecoeur.ca/programmation/montreal/philemon-cimon-jerome-miniere-solo-141119/ \"\n",
    "tokens = nltk.word_tokenize(text_expample)\n",
    "print(tokens)\n",
    "res = pos_tagger.tag(tokens)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying NLTK + Stanford pos-tag to source['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_text=source['text'][:1000]\n",
    "list_pos=[]\n",
    "for elem in list_text:\n",
    "    tokens = nltk.word_tokenize(elem)\n",
    "    res = pos_tagger.tag(tokens)\n",
    "    list_pos.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il faudrait découper le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 6242\n",
      "6242 : 12484\n",
      "12484 : 18726\n",
      "18726 : 24968\n",
      "24968 : 31210\n",
      "31210 : 37452\n",
      "37452 : 43694\n",
      "43694 : 49936\n",
      "49936 : 56178\n",
      "56178 : 62420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 6242),\n",
       " (6242, 12484),\n",
       " (12484, 18726),\n",
       " (18726, 24968),\n",
       " (24968, 31210),\n",
       " (31210, 37452),\n",
       " (37452, 43694),\n",
       " (43694, 49936),\n",
       " (49936, 56178),\n",
       " (56178, 62420)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_bounders=[elem*(source.shape[0]/10) for elem in range(1,11)]\n",
    "list_lim=[]\n",
    "j=0\n",
    "for i in range(0,len(list_bounders)):\n",
    "    print(j,\":\",list_bounders[i])\n",
    "    list_lim.append((j,list_bounders[i]))\n",
    "    j=list_bounders[i]\n",
    "list_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 400\n",
      "Wall time: 6min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "row=0\n",
    "row1=list_lim[row][0]\n",
    "row2=400\n",
    "print(row1,row2)\n",
    "source.loc[row1:row2,'pos_tag']=source.loc[row1:row2,'text'].apply(lambda x: pos_tagger.tag(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trying Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True,forceobj=True)\n",
    "def pos_tagger_token(x):\n",
    "    tokens=nltk.word_tokenize(x)\n",
    "    return tokens\n",
    "\n",
    "@jit(nopython=True,forceobj=True)\n",
    "def pos_tagger_token2(x):\n",
    "    pos_tag=pos_tagger.tag(x)\n",
    "    return pos_tag\n",
    "\n",
    "\n",
    "# @jit(nopython=True,forceobj=True)\n",
    "@jit(parallel=True,forceobj=True)\n",
    "def pos_tagger_token2(pos_tagger,x):\n",
    "    pos_tag=pos_tagger.tag(x)\n",
    "    return pos_tag\n",
    "\n",
    "# def pos_tagger_token(x):\n",
    "#     tokens=nltk.word_tokenize(x)\n",
    "#     return pos_tagger.tag(tokens)\n",
    "\n",
    "# @jit(parallel=True,forceobj=True)\n",
    "# @jit(nopython=True,forceobj=True)\n",
    "# @jit(nopython=True,forceobj=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 400\n",
      "Wall time: 2.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "row=0\n",
    "row1=list_lim[row][0]\n",
    "row2=400\n",
    "print(row1,row2)\n",
    "source.loc[row1:row2,'pos_tag']=source.loc[row1:row2,'text'].apply(lambda x: pos_tagger_token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 400\n",
      "Wall time: 7min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t0=time()\n",
    "row=0\n",
    "row1=list_lim[row][0]\n",
    "row2=400\n",
    "print(row1,row2)\n",
    "source.loc[row1:row2,'pos_tag2']=source.loc[row1:row2,'pos_tag'].apply(lambda x: pos_tagger_token2(x))\n",
    "print(time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 400\n",
      "462.839049577713 7.71398415962855\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "t0=time()\n",
    "row=0\n",
    "row1=list_lim[row][0]\n",
    "row2=400\n",
    "print(row1,row2)\n",
    "@jit(parallel=True,forceobj=True)\n",
    "source.loc[row1:row2,'pos_tag2']=source.loc[row1:row2,'pos_tag'].apply(lambda x: pos_tagger_token2(pos_tagger,x))\n",
    "print(time()-t0,(time()-t0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit(nopython=True,forceobj=True)\n",
    "@jit(parallel=True,forceobj=True)\n",
    "def pos_tag_loop(array_text,pos_tagger):\n",
    "    pos_tag_2=[]\n",
    "    for elem in array_text:\n",
    "    #     print(elem)\n",
    "        postag=pos_tagger_token2(pos_tagger,elem)\n",
    "        pos_tag_2.append(postag)\n",
    "    return pos_tag_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "array_text=source.loc[row1:row2,'pos_tag']\n",
    "pos_tag_2=pos_tag_loop(array_text,pos_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pos tag all texts : concatenate all texts in one text, and pass it to stanford tagger\n",
    "        - I tested in a small dataset 400 rows ==> it's take a few seconds instead of 7min30 when doing a loop even with numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6242"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_lim[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from array to text 0.008073091506958008 0.0001345515251159668\n",
      "tokenize text 4.5760674476623535 0.07626779079437256\n",
      "pos tag tokens 64.79420065879822 1.0799033443133037\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t0=time()\n",
    "row=0\n",
    "row1=list_lim[row][0]\n",
    "row2=800 #put3\n",
    "#18726 list_lim[2][1]\n",
    "#24968 ça ne marche pas list_lim[3][1]\n",
    "array_text=source.loc[row1:row2,'text'].values\n",
    "one_text=\" . \".join(array_text)\n",
    "print(\"from array to text\",time()-t0,(time()-t0)/60)\n",
    "\n",
    "t0=time()\n",
    "tokens=nltk.word_tokenize(one_text)\n",
    "print(\"tokenize text\",time()-t0,(time()-t0)/60)\n",
    "\n",
    "t0=time()\n",
    "pos_tag=pos_tagger.tag(tokens)\n",
    "print(\"pos tag tokens\",time()-t0,(time()-t0)/60)\n",
    "\n",
    "# tokenize text for the whole current dataset 242.13997292518616 sec 4.035682888825734 mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "from array to text 0.00299835205078125 4.99725341796875e-05\n",
      "tokenize text 1.9769279956817627 0.032948799928029376\n",
      "pos tag tokens 21.792778253555298 0.3632129708925883\n",
      "800\n",
      "from array to text 0.005970001220703125 9.950002034505208e-05\n",
      "tokenize text 2.153047800064087 0.03588413000106812\n",
      "pos tag tokens 40.29149866104126 0.671524977684021\n",
      "1200\n",
      "from array to text 0.005002260208129883 8.337100346883138e-05\n",
      "tokenize text 1.9219985008239746 0.03203330834706624\n",
      "pos tag tokens 24.845219373703003 0.4140869895617167\n",
      "1600\n",
      "from array to text 0.004996299743652344 8.327166239420573e-05\n",
      "tokenize text 2.3050007820129395 0.03841667970021566\n",
      "pos tag tokens 24.48404884338379 0.40806748072306315\n",
      "2000\n",
      "from array to text 0.04395127296447754 0.0007325212160746256\n",
      "tokenize text 2.121335744857788 0.0353555957476298\n",
      "pos tag tokens 37.48795533180237 0.6247992555300395\n",
      "2400\n",
      "from array to text 0.03299832344055176 0.0005499720573425293\n",
      "tokenize text 2.121002435684204 0.03535004059473674\n",
      "pos tag tokens 27.743045806884766 0.46238409678141273\n",
      "2800\n",
      "from array to text 0.060959577560424805 0.0010159929593404134\n",
      "tokenize text 2.0816025733947754 0.03469337622324626\n",
      "pos tag tokens 22.54000973701477 0.37566682895024617\n",
      "3200\n",
      "from array to text 0.035042762756347656 0.0005840460459391276\n",
      "tokenize text 1.943002700805664 0.03238337834676107\n",
      "pos tag tokens 22.285604000091553 0.3714267333348592\n",
      "3600\n",
      "from array to text 0.03598976135253906 0.0005998293558756511\n",
      "tokenize text 1.927964448928833 0.03213274081548055\n",
      "pos tag tokens 22.37215781211853 0.3728692968686422\n",
      "4000\n",
      "from array to text 0.03199434280395508 0.0005332390467325846\n",
      "tokenize text 2.19803786277771 0.0366339643796285\n",
      "pos tag tokens 42.49699640274048 0.708283273379008\n",
      "4400\n",
      "from array to text 0.02698373794555664 0.00044972896575927733\n",
      "tokenize text 2.076009750366211 0.03460016250610352\n",
      "pos tag tokens 25.617615938186646 0.42696026563644407\n",
      "4800\n",
      "from array to text 0.021000385284423828 0.0003500064214070638\n",
      "tokenize text 1.9410383701324463 0.032350639502207436\n",
      "pos tag tokens 20.64297652244568 0.34404960870742796\n"
     ]
    }
   ],
   "source": [
    "old_row=0\n",
    "array_tag=[]\n",
    "for i in range(400,5000,400):\n",
    "    print(i)\n",
    "    t0=time()\n",
    "    row1=old_row\n",
    "    row2=i\n",
    "    array_text=source.loc[row1:row2,'text'].values\n",
    "    one_text=\" . \".join(array_text)\n",
    "    print(\"from array to text\",time()-t0,(time()-t0)/60)\n",
    "\n",
    "    t0=time()\n",
    "    tokens=nltk.word_tokenize(one_text)\n",
    "    print(\"tokenize text\",time()-t0,(time()-t0)/60)\n",
    "\n",
    "    t0=time()\n",
    "#     pos_tagger = StanfordPOSTagger(model, jar, encoding='utf8' )\n",
    "    pos_tag=pos_tagger.tag(tokens)\n",
    "    print(\"pos tag tokens\",time()-t0,(time()-t0)/60)\n",
    "    array_tag.append(pos_tag)\n",
    "    old_row=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(10000-4800)/400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "from array to text 0.02399921417236328 0.00039998690287272137\n",
      "tokenize text 12.185036420822144 0.20308394034703572\n"
     ]
    }
   ],
   "source": [
    "old_row=4800\n",
    "for i in range(5200,10000,400):\n",
    "    print(i)\n",
    "    t1=time()\n",
    "    t0=time()\n",
    "    row1=old_row\n",
    "    row2=i\n",
    "    array_text=source.loc[row1:row2,'text'].values\n",
    "    one_text=\" . \".join(array_text)\n",
    "    print(\"from array to text\",time()-t0,(time()-t0)/60)\n",
    "\n",
    "    t0=time()\n",
    "    tokens=nltk.word_tokenize(one_text)\n",
    "    print(\"tokenize text\",time()-t0,(time()-t0)/60)\n",
    "\n",
    "    t0=time()\n",
    "#     pos_tagger = StanfordPOSTagger(model, jar, encoding='utf8' )\n",
    "    pos_tag=pos_tagger.tag(tokens)\n",
    "    print(\"pos tag tokens\",time()-t0,(time()-t0)/60)\n",
    "    array_tag.append(pos_tag)\n",
    "    old_row=i\n",
    "    \n",
    "    print(\"end one loop\",time()-t1,(time()-t1)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "there is an error when trying to run it on all data set:\n",
    "OSError: Java command failed : ['C:\\\\Program Files\\\\Java\\\\jdk1.8.0_241\\\\bin\\\\java.exe', '-mx1000m', '-cp', \n",
    "                                    'C:/Users/Hanane/Documents/nltk_stanford_pos_tagging/stanford-tagger-4.0.0/stanford-postagger-4.0.0.jar'\n",
    "                                    , 'edu.stanford.nlp.tagger.maxent.MaxentTagger', '-model', 'C:/Users/Hanane/Documents/nltk_stanford_pos_tagging/stanford-tagger-4.0.0/models/french-ud.tagger', \n",
    "                                    '-textFile', 'C:\\\\Users\\\\Hanane\\\\AppData\\\\Local\\\\Temp\\\\tmprs61ph2h', '-tokenize', 'false'\n",
    "                                    , '-outputFormatOptions', 'keepEmptySentences', '-encoding', 'utf8']\n",
    "            \n",
    "            \n",
    "Loading default properties from tagger C:/Users/Hanane/Documents/nltk_stanford_pos_tagging/stanford-tagger-4.0.0/models/french-ud.tagger\n",
    "Loading POS tagger from C:/Users/Hanane/Documents/nltk_stanford_pos_tagging/stanford-tagger-4.0.0/models/french-ud.tagger ... done [0.2 sec].\n",
    "Exception in thread \"main\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
    "\tat java.util.regex.Matcher.<init>(Matcher.java:225)\n",
    "\tat java.util.regex.Pattern.matcher(Pattern.java:1093)\n",
    "\tat edu.stanford.nlp.process.DocumentPreprocessor$PlainTextIterator.primeNext(DocumentPreprocessor.java:340)\n",
    "\tat edu.stanford.nlp.process.DocumentPreprocessor$PlainTextIterator.hasNext(DocumentPreprocessor.java:375)\n",
    "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1772)\n",
    "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1836)\n",
    "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1609)\n",
    "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1565)\n",
    "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.main(MaxentTagger.java:1908)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "special_charac=list(punctuation)\n",
    "\n",
    "# import re\n",
    "# re.split(re.compile(\"|\".join([\", \", \" \"])), \"Firstname, Lastname Email\")\n",
    "\n",
    "# >>> a='Beautiful, is; better*than\\nugly'\n",
    "# >>> import re\n",
    "# >>> re.split('; |, |\\*|\\n',a)\n",
    "# ['Beautiful', 'is', 'better', 'than', 'ugly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text_exp,special_charac):\n",
    "    tokens = nltk.word_tokenize(text_exp)\n",
    "    list_ex_nex=[]\n",
    "    for elem in tokens:\n",
    "        if len(elem)>1:\n",
    "            for char in special_charac:\n",
    "                 if char in elem:\n",
    "#                     print(elem,type(elem))\n",
    "#                     print(char)\n",
    "                    new_elem=elem.split(char)\n",
    "    #                 print(char,new_elem)\n",
    "                    nbr_char=len(new_elem)-1\n",
    "                    j=1\n",
    "                    while j<=nbr_char:\n",
    "                        new_elem.insert(j,char)\n",
    "                        j+=1\n",
    "                    elem=\" \".join(new_elem)\n",
    "        if type(elem)== list :\n",
    "            list_ex_nex.extend(elem)\n",
    "        else:\n",
    "            list_ex_nex.append(elem)\n",
    "    text_tokenized=\" \".join(list_ex_nex)\n",
    "    return text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# source['text_tokenizer']=source['text'].apply(lambda x : tokenize_text(x,special_charac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saint-Amand-Montrond <class 'str'>\n",
    "# -\n",
    "# Villeneuve-d'Ascq <class 'str'>\n",
    "# -\n",
    "# Villeneuve-d'Ascq <class 'str'>\n",
    "# -\n",
    "# 02/11/2019 <class 'str'>\n",
    "# ... <class 'str'>\n",
    "# .\n",
    "# start-up <class 'str'>\n",
    "# -\n",
    "# centre-bourg <class 'str'>\n",
    "# -\n",
    "# peut-être <class 'str'>\n",
    "# -\n",
    "# Léo-Lagrange <class 'str'>\n",
    "# -\n",
    "# Issert-Tabardel <class 'str'>\n",
    "# -\n",
    "# h. <class 'str'>\n",
    "# .\n",
    "# bar-presse-PMU <class 'str'>\n",
    "# -\n",
    "# boulangerie-drive <class 'str'>\n",
    "\n",
    "# l\\'appareil <class 'str'>\n",
    "# \\\n",
    "# l\\'intégration <class 'str'>\n",
    "# \\\n",
    "# d\\'une <class 'str'>\n",
    "# \\\n",
    "# d\\'entrée <class 'str'>\n",
    "# \\\n",
    "# d\\'un <class 'str'>\n",
    "# \\\n",
    "# d\\'empreintes <class 'str'>\n",
    "\n",
    "# E-mail <class 'str'>\n",
    "# -\n",
    "# dpgmedia.be <class 'str'>\n",
    "# .\n",
    "# 0679.994.942 <class 'str'>\n",
    "# .\n",
    "# Joint-venture <class 'str'>\n",
    "# -\n",
    "# 02/255.32.11 <class 'str'>\n",
    "# .\n",
    "# 02/255 . . 32 11 <class 'str'>\n",
    "# /\n",
    "# E-mail <class 'str'>\n",
    "# -\n",
    "# dpgmedia.be <class 'str'>\n",
    "# .\n",
    "# 0432.306.234 <class 'str'>\n",
    "# .\n",
    "# 02/255.32.11 <class 'str'>\n",
    "\n",
    "# anti-performeur <class 'str'>\n",
    "# -\n",
    "# explique-t-il <class 'str'>\n",
    "# -\n",
    "# faux-semblants <class 'str'>\n",
    "# -\n",
    "# //coupdecoeur.ca/programmation/montreal/philemon-cimon-jerome-miniere-solo-141119/ <class 'str'>\n",
    "# -\n",
    "# //coupdecoeur.ca/programmation/montreal/philemon - - - - - cimon jerome miniere solo 141119/ <class 'str'>\n",
    "# .\n",
    "# //coupdecoeur . ca/programmation/montreal/philemon - - - - - cimon jerome miniere solo 141119/ <class 'str'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
