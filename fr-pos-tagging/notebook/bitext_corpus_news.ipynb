{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metaData</th>\n",
       "      <th>hashSignature</th>\n",
       "      <th>title</th>\n",
       "      <th>region</th>\n",
       "      <th>domain</th>\n",
       "      <th>media</th>\n",
       "      <th>mobile</th>\n",
       "      <th>index_date</th>\n",
       "      <th>category</th>\n",
       "      <th>boost</th>\n",
       "      <th>...</th>\n",
       "      <th>published_date</th>\n",
       "      <th>description</th>\n",
       "      <th>body</th>\n",
       "      <th>host</th>\n",
       "      <th>qrank</th>\n",
       "      <th>country</th>\n",
       "      <th>url</th>\n",
       "      <th>lastmod</th>\n",
       "      <th>slug</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>Comment berner un système de reconnaissance d’...</td>\n",
       "      <td></td>\n",
       "      <td>sciencesetavenir.fr</td>\n",
       "      <td>https://www.sciencesetavenir.fr/assets/img/201...</td>\n",
       "      <td>https://www.sciencesetavenir.fr/high-tech/inte...</td>\n",
       "      <td>1572792800</td>\n",
       "      <td>tech</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1572789600</td>\n",
       "      <td>Faire passer un panda pour un gibbon, transfor...</td>\n",
       "      <td>Comment berner un système de reconnaissance d’...</td>\n",
       "      <td>sciencesetavenir.fr</td>\n",
       "      <td>14231</td>\n",
       "      <td>fr</td>\n",
       "      <td>https://www.sciencesetavenir.fr/high-tech/inte...</td>\n",
       "      <td>2019-11-03 14:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Comment berner un système de reconnaissance d’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>Le grand huit de Christopher Bell au Texas</td>\n",
       "      <td></td>\n",
       "      <td>us-racing.com</td>\n",
       "      <td>http://www.us-racing.com/wp-content/uploads/20...</td>\n",
       "      <td>http://www.us-racing.com/2019/11/03/le-grand-h...</td>\n",
       "      <td>1572792801</td>\n",
       "      <td>sports</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1572788997</td>\n",
       "      <td>FORT WORTH, TEXAS – NOVEMBER 02: Christopher B...</td>\n",
       "      <td>NHRA En remportant sa huitième victoire de la ...</td>\n",
       "      <td>us-racing.com</td>\n",
       "      <td>408501</td>\n",
       "      <td>fr</td>\n",
       "      <td>http://www.us-racing.com/2019/11/03/le-grand-h...</td>\n",
       "      <td>2019-11-03 14:02:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Le grand huit de Christopher Bell au Texas. NH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  metaData hashSignature                                              title  \\\n",
       "0       {}            []  Comment berner un système de reconnaissance d’...   \n",
       "1       {}            []         Le grand huit de Christopher Bell au Texas   \n",
       "\n",
       "  region               domain  \\\n",
       "0         sciencesetavenir.fr   \n",
       "1               us-racing.com   \n",
       "\n",
       "                                               media  \\\n",
       "0  https://www.sciencesetavenir.fr/assets/img/201...   \n",
       "1  http://www.us-racing.com/wp-content/uploads/20...   \n",
       "\n",
       "                                              mobile  index_date category  \\\n",
       "0  https://www.sciencesetavenir.fr/high-tech/inte...  1572792800     tech   \n",
       "1  http://www.us-racing.com/2019/11/03/le-grand-h...  1572792801   sports   \n",
       "\n",
       "   boost  ...  published_date  \\\n",
       "0    1.0  ...      1572789600   \n",
       "1    1.0  ...      1572788997   \n",
       "\n",
       "                                         description  \\\n",
       "0  Faire passer un panda pour un gibbon, transfor...   \n",
       "1  FORT WORTH, TEXAS – NOVEMBER 02: Christopher B...   \n",
       "\n",
       "                                                body                 host  \\\n",
       "0  Comment berner un système de reconnaissance d’...  sciencesetavenir.fr   \n",
       "1  NHRA En remportant sa huitième victoire de la ...        us-racing.com   \n",
       "\n",
       "    qrank  country                                                url  \\\n",
       "0   14231       fr  https://www.sciencesetavenir.fr/high-tech/inte...   \n",
       "1  408501       fr  http://www.us-racing.com/2019/11/03/le-grand-h...   \n",
       "\n",
       "               lastmod slug                                               text  \n",
       "0  2019-11-03 14:00:00  NaN  Comment berner un système de reconnaissance d’...  \n",
       "1  2019-11-03 14:02:38  NaN  Le grand huit de Christopher Bell au Texas. NH...  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "Path = \"C:\\\\Users\\\\Hanane\\\\Documents\\\\Python_Scripts\\\\TelecomParis\\\\Qwant\\\\dataset\\\\big_file_split\"\n",
    "file_name=\"month_2019_11_1.json\"\n",
    "df=pd.read_json(Path + \"\\\\\"+file_name, lines=True)\n",
    "source=pd.DataFrame.from_records(df['_source'])\n",
    "source['text']=source['title']+\". \"+source['body']\n",
    "# source.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods: Tokenization + bitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitext_file(data,row_max=400):\n",
    "    all_corpus=[]\n",
    "    word_cut=\".\"\n",
    "    for text in data[0:row_max]:\n",
    "        if text[-1]!=word_cut:\n",
    "            text+=word_cut\n",
    "        list_tokens = nltk.word_tokenize(text)\n",
    "        sentence_words=[]\n",
    "        all_sentences_words=[]\n",
    "\n",
    "        for elem in list_tokens:\n",
    "#             if elem==word_cut and elem !='\\ufeff':\n",
    "            if elem==word_cut:\n",
    "                sentence_words.append(elem)\n",
    "                all_sentences_words.append(' '.join(sentence_words))\n",
    "                sentence_words=[]\n",
    "#             if elem!=word_cut and elem !='\\ufeff':\n",
    "            if elem!=word_cut:\n",
    "                sentence_words.append(elem)\n",
    "        all_corpus.extend(all_sentences_words)\n",
    "    return all_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt(sentences_array,file_output_name):\n",
    "    if os.path.isfile(file_output_name):\n",
    "        os.remove(file_output_name)\n",
    "    with open(file_output_name, \"a\",errors=\"ignore\") as a_file:\n",
    "        for elem in sentences_array:\n",
    "            a_file.write(elem)\n",
    "            a_file.write(\"\\n\")\n",
    "    a_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tokenization + Bitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "row_max=10000\n",
    "all_corpus=bitext_file(source['text'],row_max=row_max)\n",
    "save_to_txt(all_corpus,\"news_words_bitext_\"+str(row_max)+\".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "row_max=6000\n",
    "all_corpus=bitext_file(source['text'],row_max=row_max)\n",
    "save_to_txt(all_corpus,\"news_words_bitext_\"+str(row_max)+\".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 58.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_corpus=[]\n",
    "word_cut=\".\"\n",
    "max_row=10000\n",
    "for text in source['text'][0:max_row]:\n",
    "    if text[-1]!=word_cut:\n",
    "        text+=word_cut\n",
    "    list_words = nltk.word_tokenize(text)\n",
    "    sentence_words=[]\n",
    "    all_sentences_words=[]\n",
    "    cpt_all=0\n",
    "    cpt_file=len(list_words)\n",
    "\n",
    "    for elem in list_words:\n",
    "        if elem==word_cut and elem !='\\ufeff':\n",
    "            sentence_words.append(elem)\n",
    "            all_sentences_words.append(' '.join(sentence_words))\n",
    "            sentence_words=[]\n",
    "            cpt_all+=1\n",
    "        if elem!=word_cut and elem !='\\ufeff':\n",
    "            sentence_words.append(elem)\n",
    "            cpt_all+=1\n",
    "#         if cpt_all==cpt_file:\n",
    "#             sentence_words.append(elem)\n",
    "#             all_sentences_words.append(' '.join(sentence_words))\n",
    "    all_corpus.extend(all_sentences_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
